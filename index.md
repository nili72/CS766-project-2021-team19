---
layout: default
---
# Synthetic CT generation from PET images for attenuation correction with Deep Learning methods
### CS766 2022 Spring project
### Xue Li, Ni Li

## Introduction and Motivation 

Positron emission tomography (PET) and positron emission mammography (PEM) using 18Ffluorodeoxyglucose
(FDG) have been shown to have high specificity for breast cancer detection.
However, breast-specific PET is not routinely utilized; instead, magnetic resonance imaging (MRI) is used clinically for extent of disease evaluation and neoadjuvant therapy response assessment for patients with newly diagnosed breast cancer. The recent availability of simultaneous PET/MR scanners offers the benefit of combining the high sensitivity for the detection of breast cancer provided by contrast-enhanced MR imaging with functional imaging using PET.

However, there are two problems for MR images generated by clinical PET/MR system, the missing bone information and back truncation problem as shown in Fig 1. The former one is due to the technical challenges of obtaining positive MR signal in bone in clinical PET/MR system while the latter one is caused by position patient are placed during imaging to get better breast images. Both of two problems will lead to errors during breast cancer treatment, which is not desired. In current pipeline, MR images are used to generate synthetic CT (sCT) images for later attenuation correction of PET images. Therefore, the missing bone and back truncation problems
will happen to sCT as well. As a result, inaccurate attenuation correction of PET images is calculated, and there is up to 20% error for the final treatment plan.

As the pipeline shows, the key to solve this problem is to get accurate sCT images with bone and back information. Recent advances in deep learning (DL) have demonstrated great success in accomplishing image processing tasks, like image transfer, converting images from one domain to anther domain. These methods have
been applied to medical imaging successfully, like synthetic CT generation from MR images. Therefore, we want to train a DL model with breast dataset to get accurate sCT images directly from PET images which donâ€™t have such a problem mentioned.


## Project Proposal

https://github.com/nili72/CS766-project-2022-team19/blob/8af7fd457c0f37dfafecd042d10a0d64a67d8518/Report/CS766_ProjectProposal.pdf

## Midterm Report 

https://github.com/nili72/CS766-project-2022-team19/blob/8af7fd457c0f37dfafecd042d10a0d64a67d8518/Report/CS766_midterm_report.pdf

## Presentation video
<p align="center">
<iframe width="560" height="315" src="https://drive.google.com/drive/folders/1pvh_KKiJCi5xug-S3EHWnjHVQ9QIS3DP?usp=sharing" allowfullscreen></iframe>
</p>

## Project code repo
https://github.com/nili72/CS766-project-2022-team19/tree/gh-pages/Code

## Project Timeline

| When                 | Task                                               | 
|:---------------------|:---------------------------------------------------|
| Before Feb 24        | Project Proposal and the initial webpage           | 
| Feb 25 - Mar 10      | Create DQN Miniworld benchmark                     | 
| Mar 11- Mar 20       | Set up SLAM with Miniworld                         |
| Mar 21- Apr 6        | Design input encoding for SLAM features into       | 
| Apr 7 - Apr 21       | Contrast performance of DQN on Miniworld           | 
| Before May 5         | Complete project writeup and presentation          | 

## Current State of Art
Recently, a number of studies have used deep learning and convolutional neural networks to create sCT images[1-3]. Among the many kinds of convolutional neural networks, the Unet[4] has shown outstanding performance in medical image segmentation and synthesis. More recently, generative adversarial networks (GANs)[5] have become popular in creating realistic synthetic images. sCT images of pelvic, liver, brain, and head and neck regions have been produced by GANs and their variants[6-10]. However, current studies focus more on MR-to-CT than PET-to-CT due to the potential problems mentioned in the introduction section, and very few studies show the transfer from PET images to sCT images [11]. Also, these studies were conducted based on the brain pelvis head-and-neck, abdomen datasets in which there is no truncation problem for both MR and CT images. This implies the challenges of generating sCT directly from PET for the breast area.
<p align="center">
<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/762a0d12f0c8c96c22adf95c333657b923e9f455/images/MRI-CT.png">
</p>

<p align="center">
<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/762a0d12f0c8c96c22adf95c333657b923e9f455/images/PET-CT.png">
</p>
In this project, we used Unet and UnetR[12] model to synthesis CT images directly from
PET images. The sCT images should be accurate enough to have lower error than sCT generated
from current pipeline. The official code for UnetR is pytorch, and it is used for medical image
segmentation. We will play with it first, and then try to modify it to complete the PET-to-CT image
generation task.


## Dataset and Preprocess
The breast dataset contains 23 subjects in total, including CT images and PET images. CT images have the
size of 192x192x47 with spacing information (3.646mm, 3.646mm, 3.27mm) while PET images have the size of 192x192x89 with spacing information (3.125mm, 3.125mm, 3.78mm).

The noise of the CT images and PET images were removed firstly. Then, PET images were registered to CT images to have the same data size and spacing information. The HU value of CT images is cropped into the range of [-1000, 2000] according to the meaning of HU value. Both CT and PET images are normalized. The normalization for CT is shown as follows:
<p align="center">
<img width="1000" src="https://github.com/nili72/CS766-project-2022-team19/blob/2f891823f81c35f9379efb99ef5e7ab5e001325c/images/process-equ.png">
</p>
The normalization for PET is simple. Just divide by 1500. Since not all the CT images have perfect back images, we cropped the imperfect back area for both CT and PET images.
## Approach
There are 24 cases in the breast dataset. We used 18 cases for training, 3 for validation, and 3 for testing. 8-folder cross validation was conducted to produce accurate evaluation .

Unet and GAN-related models are the most common models in the field of sCT generation. Here, we developed a 4-layer patch-based 3D Unet model for this task as Fig.3 shows. The patch size we used for training is 64x64x32. The input is PET images with cube size (64x64x32), and the ground truth is the corresponding CT images with same cube size. 
Three different loss functions were tried, including MAE, MSE, and perceptual loss. The first two are pixel-based while the last one is feature and style based.

<p align="center">
<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/bef3d28dc798187fb680e3edfb318430de11915d/images/UNet.png">
</p>

The pipeline of our study is shown in Figure 4. First, PET images were registered to CT images. And then data processing methods were applied to both PET and CT images. After that, processed PET and CT images were used for training. The vgg part is especially for perceptual loss. Pretrained VGG was used to capture the difference between sCT and CT, so that perceptual loss can be calculated to improve the model. Once the sCT was predicted, it was used for PET/CT reconstruction to get the corrected PET for later dose calculation in different tumor area. Finally, the absolute percent error between dose calculation from CT-based and sCT-based reconstructed PET was computed for final evaluation.

<p align="center">
<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/bef3d28dc798187fb680e3edfb318430de11915d/images/Pipeline.png">
</p>




## Results
The sCT images generated by each model is shown as Fig. 5. All those sCT images look good, and the UNet
with mae loss model produces better tissue contrast than the other two models.
<p align="center">
<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/6f07517e8ad88e586ab5479a8a4a4a220fb82d10/images/sCTs.png">
</p>
The eight-fold cross validation results are shown in Fig. 6. No obvious difference were found
between them. The absolute percent error for each tumor is shown as Table. 1.
UNet with mae model out-performs the current pipeline (petmr) in the Liver and Blood area, but for
the breast and FGT area, current pipeline produced better results.
<p align="center">
<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/6f07517e8ad88e586ab5479a8a4a4a220fb82d10/images/errors.png">
</p>

<img width="800" src="https://github.com/nili72/CS766-project-2022-team19/blob/effea206ad63fbc9fb9bc5381a7b229d7f9ad531/images/dose%20calc.png">
</p>



## Discussion


## References




