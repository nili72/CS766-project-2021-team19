---
layout: default
---
# Synthetic CT generation from PET images for attenuation correction with Deep Learning methods
### CS766 2022 Spring project
### Xue Li, Ni Li

## Introduction and Motivation 

Positron emission tomography (PET) and positron emission mammography (PEM) using 18Ffluorodeoxyglucose
(FDG) have been shown to have high specificity for breast cancer detection.
However, breast-specific PET is not routinely utilized; instead, magnetic resonance imaging (MRI) is used clinically for extent of disease evaluation and neoadjuvant therapy response assessment for patients with newly diagnosed breast cancer. The recent availability of simultaneous PET/MR scanners offers the benefit of combining the high sensitivity for the detection of breast cancer provided by contrast-enhanced MR imaging with functional imaging using PET.

However, there are two problems for MR images generated by clinical PET/MR system, the missing bone information and back truncation problem as shown in Fig 1. The former one is due to the technical challenges of obtaining positive MR signal in bone in clinical PET/MR system while the latter one is caused by position patient are placed during imaging to get better breast images. Both of two problems will lead to errors during breast cancer treatment, which is not desired. In current pipeline, MR images are used to generate synthetic CT (sCT) images for later attenuation correction of PET images. Therefore, the missing bone and back truncation problems
will happen to sCT as well. As a result, inaccurate attenuation correction of PET images is calculated, and there is up to 20% error for the final treatment plan.

As the pipeline shows, the key to solve this problem is to get accurate sCT images with bone and back information. Recent advances in deep learning (DL) have demonstrated great success in accomplishing image processing tasks, like image transfer, converting images from one domain to anther domain. These methods have
been applied to medical imaging successfully, like synthetic CT generation from MR images. Therefore, we want to train a DL model with breast dataset to get accurate sCT images directly from PET images which donâ€™t have such a problem mentioned.


## Project Proposal

https://github.com/nili72/CS766-project-2022-team19/blob/8af7fd457c0f37dfafecd042d10a0d64a67d8518/Report/CS766_ProjectProposal.pdf

## Midterm Report 

https://github.com/nili72/CS766-project-2022-team19/blob/8af7fd457c0f37dfafecd042d10a0d64a67d8518/Report/CS766_midterm_report.pdf

## Presentation video
<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/2P8yhjrFa4E" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

## Project code repo
https://github.com/nili72/CS766-project-2022-team19/tree/gh-pages/Code

## Project Timeline

| When                 | Task                                               | 
|:---------------------|:---------------------------------------------------|
| Before Feb 24        | Project Proposal and the initial webpage           | 
| Feb 25 - Mar 10      | Create DQN Miniworld benchmark                     | 
| Mar 11- Mar 20       | Set up SLAM with Miniworld                         |
| Mar 21- Apr 6        | Design input encoding for SLAM features into       | 
| Apr 7 - Apr 21       | Contrast performance of DQN on Miniworld           | 
| Before May 5         | Complete project writeup and presentation          | 

## Current State of Art
Recently, a number of studies have used deep learning and convolutional neural networks to create
sCT images[1-3]. Among the many kinds of convolutional neural networks, the Unet[4] has shown
outstanding performance in medical image segmentation and synthesis. More recently, generative
adversarial networks (GANs)[5] have become popular in creating realistic synthetic images. sCT
images of pelvic, liver, brain, and head and neck regions have been produced by GANs and their
variants[6-10]. However, almost all the studies got sCT from MR images, and very few studies
show the transfer from PET images to sCT images [11]. Specifically, no research has been done to
get sCT images from PET images in the breast tumor region which requires higher accuracy.
In this project, we will try to use Unet and UnetR[12] model to synthesis CT images directly from
PET images. The sCT images should be accurate enough to have lower error than sCT generated
from current pipeline. The official code for UnetR is pytorch, and it is used for medical image
segmentation. We will play with it first, and then try to modify it to complete the PET-to-CT image
generation task.

## Dataset and Preprocess
The breast dataset contains 23 subjects in total, including CT images and PET images. CT images have the
size of 192x192x47 with spacing information (3.646mm, 3.646mm, 3.27mm) while PET images have the size of 192x192x89 with spacing information (3.125mm, 3.125mm, 3.78mm).

The noise of the CT images and PET images were removed firstly. Then, PET images were registered to CT images to have the same data size and spacing information. The HU value of CT images is cropped into the range of [-1000, 2000] according to the meaning of HU value. Both CT and PET images are normalized. The normalization for CT is shown as follows:
<p align="center">
<img width="1000" src="https://github.com/nili72/CS766-project-2022-team19/blob/2f891823f81c35f9379efb99ef5e7ab5e001325c/images/process-equ.png">
</p>
The normalization for PET is simple. Just divide by 1500. Since not all the CT images have perfect back images, we cropped the imperfect back area for both CT and PET images.
## Approach
A 4-layer 3D UNet model was used for the training. The input is PET images with cube size
(64x64x32), and the ground truth is the corresponding CT images with same cube size. The network structure is shown below.

<p align="center">
<img width="500" src="https://raw.githubusercontent.com/cmilica/cs766project/gh-pages/assets/hallway.png">
</p>

- Environment setting:
- Simulation environment: gym-miniworld
- SLAM library: 
  - pySLAM
  - ORB-SLAM2 + python bindings
  - ORB-SLAM3 (did not work well)
- Reinforcement learning library : Stable Baselines

### ORB-SLAM2

It's worth mentioning that most SLAM library are designed for ROS and C++ platform, such as orb-SLAM2 and lsd-SLAM, therefore we spent great effort in letting SLAM running on a python simulation environment without ROS.

- Modify step() function where each step action is defined in gym-miniworld so that it will call orb-slam2 directly
- Modify the python-binding for processMono() so that it will directly output 4x4 homogenous transformation matrix which contain position and orientation.
- Interpolate each action into 10 frames and feed into SLAM
- Camera calibration:
   - OpenGL has some different way of defining the projection matrix and use angle of view to represent intrinsic camera information.
- Focal length:
- Alpha = 2 arctan(d/2f)

    where d is the dimension of image, alpha is the angle of view, f is the focus length.
- Principal points: 
   - camera set by gluperspective() function requires zero principle when map into normalized device coordinates(NDC), but since we export the rendering and fed that into SLAM as a OpenCV image, the principal points then becomes the center of image

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/M9-AGSCgOJ0" tframeborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

### PySLAM

In addition to Orb-SLAM 2, we used a modified version of pyslam to extract position and orientation. 
We reconfigured pyslam to accept an image as an argument and return the position and orientation quaternion. 
Pyslam also provides a monocular SLAM implementation, so it forms its localization and mapping estimates from a sequence of single-camera RGB images

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ofq9iDRJG6I" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>


### ORB-SLAM3

ORB-SLAM3 is the latest in the ORB-SLAM libraries. 
There was a possibility that certain disadvantages of the other two libraries, like speed, and sudden map disruptions,
would be resolved with ORB-SLAM3. 
After multiple attempts to make this work, and trying to modify some of the python bindings,
we were unable to implement this approach. 

However, we did do some background research how this library would work, so we provide a brief summary of some novelties and advantages.

<p align="center">
<img width="300" src="https://raw.githubusercontent.com/cmilica/cs766project/gh-pages/assets/ORB-SLAM3.png">
</p>

Atlas is a multi-map representation composed of a set of disconnected maps.  
The tracking threadtakes input from the correct frame with respect to the active map in Atlas in real-time, 
and it decided whether the currentframe  becomes  a  keyframe.   
If  tracking  is  lost,  the  thread  tries  to  relocate  the  current  thread.   
Depending  on  that,  it decides whether the tracking is resumed, switching the active map if needed, 
or whether or not the active map is storedas non-active,  and a new map is initiated as an active map.  
The next component is a local mapping thread that adds keyframes and points to the active map, removes the redundant ones, 
and refines the map using visual or visual-inertialbundle adjustment. 
Also, in the inertial case, the IMU parameters are initialized and refined by the mapping thread usingthe MAP-estimation technique.  
The last component is the loop and map merging thread that detects common regions between the active map and the whole Atlas at keyframe rate.  
The way this works is that the system checks whetherthe common area belongs to the active map.  
The system then performs loop correction; if it belongs to a different map,both maps are seamlessly merged into a single one, 
and this becomes a new active map.  
Finally, a full BA runs as anindependent thread to prevent anything affecting a real-time map performance.

## Results

### ORB-SLAM2

Here is a demo video that pytorch is trying the agent with SLAM results in real-time. 

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/iari7YP6ovI" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

Timing issue:

| Settings                       | Time per action | 
|:-------------------------------|:----------------|
| 10frames/actionâ€‹ With viewer    | 0.34s           | 
| 10frames/actionâ€‹ Without viewer | 0.33s           | 
| 20frames/action                | 0.67s           |
| 5frames/action                 | 0.17s           | 


Resolution: 800x600

Problems to be solved:â€‹

1) speed, SLAM module will slow down the training speedâ€‹

2) unexpected loop-closure & lost track: richness of texture inside simulation environment


### PySLAM

We created a set of 200 reference images using our agent trained through standard DQN in modified MiniWorld-Hallway-v0 environment. 
We analyzed the time it took to perform SLAM on this sequence of images in pyslam


## Future work

1) Encode the slam results into DQN model input

2) Improve the code performance to get faster performance

3) Find appropriate parameters that mitigate the "lost of track" issue
